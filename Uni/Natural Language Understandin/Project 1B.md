Using the paper [[REGULARIZING AND OPTIMIZING LSTM LANGUAGE MODELS.pdf]]



> [!PDF|255, 208, 0] [[REGULARIZING AND OPTIMIZING LSTM LANGUAGE MODELS.pdf#page=2&annotation=512R|REGULARIZING AND OPTIMIZING LSTM LANGUAGE MODELS, p.2]]
> > In this work, we investigate a set of regularization strategies that are not only highly effective but which can also be used with no modification to existing LSTM implementations. The weightdropped LSTM applies recurrent regularization through a DropConnect mask on the hidden-tohidden recurrent weights. Other strategies include the use of randomized-length backpropagation through time (BPTT), embedding dropout, activation regularization (AR), and temporal activation regularization (TAR).

